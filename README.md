# Probabilistic Machine Learning - Final Project

![GitHub license](https://img.shields.io/github/license/AlexandrosKyr/Probabilistic-Machine-Learning-Diffusion-GaussianProcesses)
![GitHub stars](https://img.shields.io/github/stars/AlexandrosKyr/Probabilistic-Machine-Learning-Diffusion-GaussianProcesses?style=social)
![GitHub forks](https://img.shields.io/github/forks/AlexandrosKyr/Probabilistic-Machine-Learning-Diffusion-GaussianProcesses?style=social)

## Previous Work

This project explores **Denoising Diffusion Probabilistic Models (DDPMs)** and **Gaussian Processes (GPs)** for generative modeling and function fitting. It builds upon the following foundational papers:

- **Denoising Diffusion Probabilistic Models**  
  *Jonathan Ho, Ajay Jain, and Pieter Abbeel, 2020*  
  ğŸ“„ [Paper Link](https://arxiv.org/abs/2006.11239)  

- **Score-Based Generative Modeling through Stochastic Differential Equations**  
  *Yang Song et al., 2021*  
  ğŸ“„ [Paper Link](https://arxiv.org/abs/2011.13456)  

- **Improved Denoising Diffusion Probabilistic Models**  
  *Nichol and Dhariwal, 2021*  
  ğŸ“„ [Paper Link](https://arxiv.org/abs/2102.09672)  

- **Classifier-Free Diffusion Guidance**  
  *Ho and Salimans, 2022*  
  ğŸ“„ [Paper Link](https://arxiv.org/abs/2207.12598)  

ğŸ”— **Project Repository:** [https://github.com/AlexandrosKyr/Probabilistic-Machine-Learning-Diffusion-GaussianProcesses](https://github.com/AlexandrosKyr/Probabilistic-Machine-Learning-Diffusion-GaussianProcesses)  

---

## Table of Contents
- [Based on Previous Work](#-based-on-previous-work)
- [Quantitative Analysis of Diffusion Models](#-quantitative-analysis-of-diffusion-models)
- [Code & Implementation](#ï¸-code--implementation)
- [Results for Diffusion Models](#-results)
- [Function Fitting with Gaussian Processes](#-function-fitting-with-gaussian-processes)
- [Results for Gaussian Processes](#-results-gaussian-processes)
---

## Quantitative Analysis of Diffusion Models

This study explores **variations and extensions** of Denoising Diffusion Probabilistic Models (DDPMs), focusing on:
- **Parameterizations**: Comparing target parameterizations (`Ïµ`, `Âµ`, `x0`)  
- **Guided Diffusion**: Classifier-guided and classifier-free diffusion  
- **Continuous-Time Models**: Variance-preserving (VP), variance-exploding (VE), and sub-variance-preserving (SubVP) stochastic differential equations (SDEs)  

---

## Code & Implementation
This project builds on **UNET-based diffusion models** and extends them with **guided generation** and **continuous-time formulations**. 

âœ… **Modified DDPM implementations** for `Ïµ`, `Âµ`, `x0`  
âœ… **Added Classifier-Free and Classifier-Guided Diffusion**  
âœ… **Implemented VP-SDE, VE-SDE, and SubVP-SDE**  
âœ… **Extended Gaussian Processes for constrained function fitting**  

---

## Results

### **Quantitative Performance Comparison of Diffusion Models**

| **Method**              | **FID â†“**          | **Inception Score â†‘** | **Log-Likelihood (bits/dim) â†“** | **Training Time (minutes) â†“** |
|-------------------------|-------------------:|----------------------:|-------------------------------:|----------------------------:|
| **Base DDPM**          | 59.69 Â± 0.91       | 2.09 Â± 0.04           | -23.7633 Â± 0.3650              | 09:59                       |
| **DDPM Âµ**             | 73.86 Â± 0.69       | 2.07 Â± 0.02           | -26.1582 Â± 0.7096              | 10:26                       |
| **DDPM x0**            | 117.58 Â± 0.75      | 2.23 Â± 0.05           | -99.6665 Â± 8.6140              | 10:02                       |
| **VP SDE**             | 201.54 Â± 8.92      | 2.09 Â± 0.02           | 4.86                            | 08:27                       |
| **VE SDE**             | 164.53 Â± 6.45      | 1.95 Â± 0.03           | 4.66                            | 08:25                       |
| **Sub-VP SDE**         | 329.71 Â± 11.28     | 1.50 Â± 0.01           | 7.11                            | 08:32                       |
| **Classifier Guided**  | 55.26 Â± 1.22       | 2.11 Â± 0.02           | -24.5517 Â± 0.3285              | 09:58                       |
| **Classifier-Free**    | 56.10 Â± 0.62       | 2.07 Â± 0.03           | -24.3048 Â± 0.2923              | 11:58                       |

 **Key Observations**:  
- **Classifier-guided diffusion produces higher-quality samples** with better **FID scores**.  
- **Classifier-free diffusion** maintains diversity but performs slightly worse than classifier-guided.  
- **Continuous-time SDE models (VP/VE/SubVP)** show promise for **more structured generative models**, but computational costs remain high.  

## Function Fitting with Gaussian Processes (GPs)

In addition to diffusion models, this project explores **Gaussian Processes (GPs)** for function fitting with constraints.

### **Tasks Studied**
1. **Standard GP Regression**  
   - Compared Maximum A Posteriori (MAP) estimation and No-U-Turn Sampler (NUTS) for kernel hyperparameter tuning.
   - Evaluated predictive performance and uncertainty quantification.

2. **Gaussian Processes with Integral Constraints**  
   - Derived a **constrained posterior distribution** that satisfies integral constraints.
   - Verified theoretical properties (e.g., normality of the constrained distribution).

### **Results for Gaussian Processes**

| **Method**  | **Kernel Lengthscale** | **Kernel Variance** | **Test Log-Likelihood â†‘** |
|------------|----------------------:|-------------------:|--------------------------:|
| **MAP GP** | 0.066                 | 1.641 Â± 0.064      | **0.394 Â± 3.821**        |
| **NUTS GP** | 0.066                 | 1.842 Â± 0.100      | **-2.345 Â± 4.907**       |

**Key Findings:**
- **MAP outperformed NUTS** in test log-likelihood but had higher variance.
- **NUTS provided a broader posterior exploration**, useful for uncertainty quantification.
- **Integral constraints reduced uncertainty in GP predictions**, improving interpretability.
